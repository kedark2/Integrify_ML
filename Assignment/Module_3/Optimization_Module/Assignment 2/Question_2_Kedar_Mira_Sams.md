
# Backpropagation
Backpropagation (BP) is the central mechanism through which a neural network learns. It tells the network has made either a wrong prediction or not. Propagation means transmission of something like light, sound, motion, information in a particular direction through a particular medium from the parents. Whereas Backpropagation in neural network indicates the transmission of information, and that information relates to the error produced by the neural network when it makes a prediction about the data. Backpropagation refers to the whole process of training an artificial neural network using multiple backpropagation steps, each of which computes gradients and uses them to perform a Gradient Descent step. In contrast, reverse-mode auto diff is simply a technique used to compute gradients efficiently and it happens to be used by backpropagation.
# Automatic Differentiation
Automatic Differentiation is applied in deep learning and elsewhere (finance, meteorology …) to quickly compute the many differentials of a scalar function (function that computes one result) of many inputs.
Backpropagation is a special case of auto-differentiation combined with gradient descent. The goal of BP is to optimize the loss function by finding good/optimal weights values. The BP works by calculating the gradient of the loss function and propagated back to all weights. The BP uses Automatic differentiation to calculate the gradient automatically. Reverse-mode automatic differentiation works in a similar "philosophy" as backpropagation, it computes first all the values (feed forward phase) and then goes back to compute all the partial derivatives at once.
